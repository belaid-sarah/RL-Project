{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rapport de Projet : Apprentissage par Renforcement\n",
        "\n",
        "**Matière :** Apprentissage par Renforcement  \n",
        "**Objectif :** Implémenter et comprendre les algorithmes classiques de RL  \n",
        "**Date :** [À compléter]\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction\n",
        "\n",
        "### 1.1 Objectifs du Projet\n",
        "\n",
        "Ce projet vise à :\n",
        "- Implémenter les algorithmes classiques de l'apprentissage par renforcement\n",
        "- Développer plusieurs environnements pour tester ces algorithmes\n",
        "- Comparer les performances des différents algorithmes\n",
        "- Comprendre quand utiliser chaque type d'algorithme\n",
        "\n",
        "### 1.2 Structure du Projet\n",
        "\n",
        "Le projet est organisé en deux bibliothèques principales :\n",
        "- **`algos/`** : Bibliothèque d'algorithmes RL\n",
        "- **`envs/`** : Bibliothèque d'environnements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Charger les résultats des tests\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Charger les résultats JSON depuis le dossier results/\n",
        "results_dir = Path('results')\n",
        "all_results = []\n",
        "\n",
        "if results_dir.exists():\n",
        "    for json_file in results_dir.glob('*.json'):\n",
        "        if 'complete_report' not in json_file.name:\n",
        "            with open(json_file, 'r') as f:\n",
        "                result = json.load(f)\n",
        "                if result.get('success', False):\n",
        "                    all_results.append(result)\n",
        "    \n",
        "    print(f\"{len(all_results)} résultats chargés\")\n",
        "else:\n",
        "    print(\"Le dossier results/ n'existe pas. Exécutez d'abord test_all_algos_envs.py\")\n",
        "    all_results = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Résultats par Environnement\n",
        "\n",
        "### 3.1 LineWorldSimple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filtrer les résultats pour LineWorldSimple\n",
        "lineworld_results = [r for r in all_results if 'LineWorld' in r.get('environment', '')]\n",
        "\n",
        "if lineworld_results:\n",
        "    # Créer un DataFrame pour faciliter l'analyse\n",
        "    data = []\n",
        "    for r in lineworld_results:\n",
        "        data.append({\n",
        "            'Algorithm': r['algorithm'],\n",
        "            'Mean Reward': r['evaluation']['mean_reward'],\n",
        "            'Std Reward': r['evaluation']['std_reward'],\n",
        "            'Success Rate': r['evaluation']['success_rate'] * 100,\n",
        "            'Mean Steps': r['evaluation']['mean_steps'],\n",
        "            'Training Time': r['training']['training_time']\n",
        "        })\n",
        "    \n",
        "    df_lineworld = pd.DataFrame(data)\n",
        "    df_lineworld = df_lineworld.sort_values('Mean Reward', ascending=False)\n",
        "    \n",
        "    print(\"\\n=== Résultats LineWorldSimple ===\")\n",
        "    print(df_lineworld.to_string(index=False))\n",
        "    \n",
        "    # Graphique\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Reward moyen\n",
        "    axes[0].barh(df_lineworld['Algorithm'], df_lineworld['Mean Reward'])\n",
        "    axes[0].set_xlabel('Reward Moyen')\n",
        "    axes[0].set_title('Reward Moyen par Algorithme (LineWorldSimple)')\n",
        "    axes[0].grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # Taux de succès\n",
        "    axes[1].barh(df_lineworld['Algorithm'], df_lineworld['Success Rate'])\n",
        "    axes[1].set_xlabel('Taux de Succès (%)')\n",
        "    axes[1].set_title('Taux de Succès par Algorithme (LineWorldSimple)')\n",
        "    axes[1].grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Aucun résultat trouvé pour LineWorldSimple\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 GridWorldSimple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filtrer les résultats pour GridWorldSimple\n",
        "gridworld_results = [r for r in all_results if 'GridWorld' in r.get('environment', '')]\n",
        "\n",
        "if gridworld_results:\n",
        "    data = []\n",
        "    for r in gridworld_results:\n",
        "        data.append({\n",
        "            'Algorithm': r['algorithm'],\n",
        "            'Mean Reward': r['evaluation']['mean_reward'],\n",
        "            'Std Reward': r['evaluation']['std_reward'],\n",
        "            'Success Rate': r['evaluation']['success_rate'] * 100,\n",
        "            'Mean Steps': r['evaluation']['mean_steps'],\n",
        "            'Training Time': r['training']['training_time']\n",
        "        })\n",
        "    \n",
        "    df_gridworld = pd.DataFrame(data)\n",
        "    df_gridworld = df_gridworld.sort_values('Mean Reward', ascending=False)\n",
        "    \n",
        "    print(\"\\n=== Résultats GridWorldSimple ===\")\n",
        "    print(df_gridworld.to_string(index=False))\n",
        "    \n",
        "    # Graphique\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    axes[0].barh(df_gridworld['Algorithm'], df_gridworld['Mean Reward'])\n",
        "    axes[0].set_xlabel('Reward Moyen')\n",
        "    axes[0].set_title('Reward Moyen par Algorithme (GridWorldSimple)')\n",
        "    axes[0].grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    axes[1].barh(df_gridworld['Algorithm'], df_gridworld['Success Rate'])\n",
        "    axes[1].set_xlabel('Taux de Succès (%)')\n",
        "    axes[1].set_title('Taux de Succès par Algorithme (GridWorldSimple)')\n",
        "    axes[1].grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Aucun résultat trouvé pour GridWorldSimple\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Conclusion\n",
        "\n",
        "### 5.1 Résumé des Résultats\n",
        "\n",
        "[À compléter avec vos résultats]  \n",
        "Exemple : Policy Iteration a obtenu les meilleures performances sur LineWorldSimple avec 100% de taux de succès.\n",
        "\n",
        "### 5.2 Recommandations\n",
        "\n",
        "**Pour des environnements simples avec modèle disponible :**\n",
        "- Utiliser **Policy Iteration** ou **Value Iteration**\n",
        "- Avantages : Optimal garanti, rapide\n",
        "\n",
        "**Pour des environnements sans modèle :**\n",
        "- Utiliser **Q-Learning** ou **SARSA**\n",
        "- Pour accélérer : **Dyna-Q** si on peut construire un modèle\n",
        "\n",
        "**Pour des problèmes avec épisodes clairs :**\n",
        "- **Monte Carlo** peut être approprié\n",
        "\n",
        "### 5.3 Limitations et Améliorations Possibles\n",
        "\n",
        "- Les environnements sont relativement simples (peu d'états)\n",
        "- Possibilité d'étendre à des environnements plus complexes\n",
        "- Utilisation de fonction approximation pour de grands espaces d'états\n",
        "- Deep Q-Networks (DQN) pour des environnements avec observations complexes\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
