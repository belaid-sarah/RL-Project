# Guide d'Utilisation - Projet Reinforcement Learning

Ce guide explique comment utiliser tous les algorithmes d'apprentissage par renforcement sur tous les environnements.

## üìã Table des mati√®res

1. [Structure du projet](#structure-du-projet)
2. [Algorithmes impl√©ment√©s](#algorithmes-impl√©ment√©s)
3. [Environnements disponibles](#environnements-disponibles)
4. [Utilisation de base](#utilisation-de-base)
5. [Tests complets](#tests-complets)
6. [Exemples d'utilisation](#exemples-dutilisation)
7. [Interpr√©tation des r√©sultats](#interpr√©tation-des-r√©sultats)

## üìÅ Structure du projet

```
RL-PROJECT/
‚îú‚îÄ‚îÄ algos/                    # Algorithmes d'apprentissage
‚îÇ   ‚îú‚îÄ‚îÄ base_agent.py        # Classe de base pour tous les agents
‚îÇ   ‚îú‚îÄ‚îÄ policy_iteration.py  # Policy Iteration (DP)
‚îÇ   ‚îú‚îÄ‚îÄ value_iteration.py   # Value Iteration (DP)
‚îÇ   ‚îú‚îÄ‚îÄ monte_carlo.py       # Monte Carlo (ES, On-policy, Off-policy)
‚îÇ   ‚îú‚îÄ‚îÄ sarsa.py             # SARSA (TD Learning)
‚îÇ   ‚îú‚îÄ‚îÄ q_learning.py        # Q-Learning (TD Learning)
‚îÇ   ‚îú‚îÄ‚îÄ expected_sarsa.py    # Expected SARSA (TD Learning)
‚îÇ   ‚îú‚îÄ‚îÄ dyna_q.py            # Dyna-Q (Planning)
‚îÇ   ‚îî‚îÄ‚îÄ dyna_q_plus.py       # Dyna-Q+ (Planning)
‚îú‚îÄ‚îÄ envs/                     # Environnements
‚îÇ   ‚îú‚îÄ‚îÄ lineworld.py         # Line World
‚îÇ   ‚îú‚îÄ‚îÄ gridworld.py         # Grid World
‚îÇ   ‚îú‚îÄ‚îÄ rps.py               # Two-Round Rock Paper Scissors
‚îÇ   ‚îú‚îÄ‚îÄ monty_hall_level1.py # Monty Hall (3 portes)
‚îÇ   ‚îî‚îÄ‚îÄ monty_hall_level2.py # Monty Hall (5 portes)
‚îî‚îÄ‚îÄ test_all_algos_envs.py   # Script de test complet
```

## ü§ñ Algorithmes impl√©ment√©s

### Dynamic Programming

#### 1. Policy Iteration
- **Fichier**: `algos/policy_iteration.py`
- **Classe**: `PolicyIteration`
- **Description**: Alterne entre √©valuation de politique et am√©lioration jusqu'√† convergence
- **Hyperparam√®tres**:
  - `gamma` (0.99): Facteur d'actualisation
  - `theta` (1e-5): Seuil de convergence

#### 2. Value Iteration
- **Fichier**: `algos/value_iteration.py`
- **Classe**: `ValueIteration`
- **Description**: Calcule directement la fonction de valeur optimale
- **Hyperparam√®tres**:
  - `gamma` (0.99): Facteur d'actualisation
  - `theta` (1e-5): Seuil de convergence

### Monte Carlo Methods

#### 3. Monte Carlo ES (Exploring Starts)
- **Fichier**: `algos/monte_carlo.py`
- **Classe**: `MonteCarloES`
- **Description**: Monte Carlo avec exploring starts
- **Hyperparam√®tres**:
  - `gamma` (0.99): Facteur d'actualisation
  - `epsilon` (0.1): Taux d'exploration

#### 4. On-policy First Visit Monte Carlo
- **Fichier**: `algos/monte_carlo.py`
- **Classe**: `OnPolicyMonteCarlo`
- **Description**: Monte Carlo on-policy avec first-visit
- **Hyperparam√®tres**:
  - `gamma` (0.99): Facteur d'actualisation
  - `epsilon` (0.1): Taux d'exploration

#### 5. Off-policy Monte Carlo
- **Fichier**: `algos/monte_carlo.py`
- **Classe**: `OffPolicyMonteCarlo`
- **Description**: Monte Carlo off-policy avec importance sampling
- **Hyperparam√®tres**:
  - `gamma` (0.99): Facteur d'actualisation
  - `epsilon` (0.1): Taux d'exploration

### Temporal Difference Learning

#### 6. SARSA
- **Fichier**: `algos/sarsa.py`
- **Classe**: `SARSAAgent`
- **Description**: Algorithme on-policy de TD Learning
- **Hyperparam√®tres**:
  - `alpha` (0.1): Taux d'apprentissage
  - `gamma` (0.99): Facteur d'actualisation
  - `epsilon` (0.1): Taux d'exploration

#### 7. Q-Learning
- **Fichier**: `algos/q_learning.py`
- **Classe**: `QLearningAgent`
- **Description**: Algorithme off-policy de TD Learning
- **Hyperparam√®tres**:
  - `alpha` (0.1): Taux d'apprentissage
  - `gamma` (0.99): Facteur d'actualisation
  - `epsilon` (0.1): Taux d'exploration

#### 8. Expected SARSA
- **Fichier**: `algos/expected_sarsa.py`
- **Classe**: `ExpectedSARSAAgent`
- **Description**: Variante de SARSA utilisant l'esp√©rance
- **Hyperparam√®tres**:
  - `alpha` (0.1): Taux d'apprentissage
  - `gamma` (0.99): Facteur d'actualisation
  - `epsilon` (0.1): Taux d'exploration

### Planning

#### 9. Dyna-Q
- **Fichier**: `algos/dyna_q.py`
- **Classe**: `DynaQAgent`
- **Description**: Q-Learning + mod√®le de l'environnement pour planning
- **Hyperparam√®tres**:
  - `alpha` (0.1): Taux d'apprentissage
  - `gamma` (0.99): Facteur d'actualisation
  - `epsilon` (0.1): Taux d'exploration
  - `n_planning_steps` (5): Nombre d'√©tapes de planning

#### 10. Dyna-Q+
- **Fichier**: `algos/dyna_q_plus.py`
- **Classe**: `DynaQPlusAgent`
- **Description**: Dyna-Q avec bonus d'exploration pour environnements changeants
- **Hyperparam√®tres**:
  - `alpha` (0.1): Taux d'apprentissage
  - `gamma` (0.99): Facteur d'actualisation
  - `epsilon` (0.1): Taux d'exploration
  - `n_planning_steps` (5): Nombre d'√©tapes de planning
  - `kappa` (1e-3): Poids du bonus d'exploration
  - `tau` (1000): Temps de vie pour transitions "anciennes"

## üåç Environnements disponibles

### 1. Line World
- **Fichier**: `envs/lineworld.py`
- **Classe**: `LineWorld`
- **Description**: Environnement 1D avec obstacles, pi√®ges, r√©compenses
- **Actions**: 0=gauche, 1=droite, 2=rester, 3=sauter, 4=sprint
- **√âtat**: Position + √©nergie + cl√©s collect√©es

### 2. Grid World
- **Fichier**: `envs/gridworld.py`
- **Classe**: `GridWorld`
- **Description**: Environnement 2D avec obstacles, pi√®ges mobiles, r√©compenses
- **Actions**: 0=haut, 1=bas, 2=gauche, 3=droite
- **√âtat**: Position (x, y)

### 3. Two-Round Rock Paper Scissors
- **Fichier**: `envs/rps.py`
- **Classe**: `TwoRoundRPS`
- **Description**: 2 rounds de Pierre-Papier-Ciseaux
- **Actions**: 0=Rock, 1=Paper, 2=Scissors
- **√âtat**: Round actuel + choix pr√©c√©dents

### 4. Monty Hall Level 1
- **Fichier**: `envs/monty_hall_level1.py`
- **Classe**: `MontyHallLevel1`
- **Description**: Probl√®me de Monty Hall avec 3 portes
- **Actions**: √âtape 1: 0-2 (choisir porte), √âtape 2: 0=garder, 1=changer
- **√âtat**: √âtape + portes choisies/retir√©es

### 5. Monty Hall Level 2
- **Fichier**: `envs/monty_hall_level2.py`
- **Classe**: `MontyHallLevel2`
- **Description**: Probl√®me de Monty Hall avec 5 portes (4 actions)
- **Actions**: Variable selon le nombre de portes disponibles
- **√âtat**: √âtape + portes disponibles

## üöÄ Utilisation de base

### Exemple 1: Q-Learning sur Line World

```python
from envs.lineworld import LineWorld
from algos.q_learning import QLearningAgent

# Cr√©er l'environnement
env = LineWorld(length=10)

# Cr√©er l'agent
agent = QLearningAgent(
    env,
    alpha=0.1,
    gamma=0.99,
    epsilon=0.1
)

# Entra√Æner
agent.train(num_episodes=1000, verbose=True)

# √âvaluer
results = agent.evaluate(num_episodes=100)
print(f"Mean reward: {results['mean_reward']:.2f}")
print(f"Success rate: {results['success_rate']*100:.1f}%")

# Sauvegarder
agent.save("models/qlearning_lineworld.pkl")
```

### Exemple 2: Policy Iteration sur Grid World

```python
from envs.gridworld import GridWorld
from algos.policy_iteration import PolicyIteration

# Cr√©er l'environnement
env = GridWorld(width=5, height=5)

# Cr√©er l'agent
agent = PolicyIteration(
    env,
    gamma=0.99,
    theta=1e-5
)

# Entra√Æner
agent.train(num_episodes=100, verbose=True)

# Utiliser la politique apprise
state = env.reset()
action = agent.select_action(state, training=False)
```

### Exemple 3: Monte Carlo ES sur RPS

```python
from envs.rps import TwoRoundRPS
from algos.monte_carlo import MonteCarloES

# Cr√©er l'environnement
env = TwoRoundRPS()

# Cr√©er l'agent
agent = MonteCarloES(
    env,
    gamma=0.99,
    epsilon=0.1
)

# Entra√Æner
agent.train(num_episodes=500, verbose=True)

# √âvaluer
results = agent.evaluate(num_episodes=100)
```

## üß™ Tests complets

### Tester tous les algorithmes sur tous les environnements

```bash
# Tester tout
python test_all_algos_envs.py --all

# Tester avec sortie d√©taill√©e
python test_all_algos_envs.py --all --verbose

# Tester une combinaison sp√©cifique
python test_all_algos_envs.py --algo Q-Learning --env LineWorld --episodes 1000

# Tester avec sortie d√©taill√©e
python test_all_algos_envs.py --algo Q-Learning --env LineWorld --episodes 1000 --verbose
```

### R√©sultats

Les r√©sultats sont sauvegard√©s dans le dossier `results/`:
- Un fichier JSON par combinaison algorithme/environnement
- Un rapport complet avec tous les r√©sultats

### Format des r√©sultats

```json
{
  "algorithm": "Q-Learning",
  "environment": "LineWorld",
  "hyperparameters": {...},
  "training": {
    "num_episodes": 1000,
    "training_time": 12.34,
    "convergence_episode": 500,
    "final_mean_reward": 15.2,
    "best_reward": 20.0
  },
  "evaluation": {
    "mean_reward": 15.5,
    "std_reward": 2.3,
    "success_rate": 0.85,
    "mean_steps": 25.4
  }
}
```

## üìä Exemples d'utilisation

### Comparer plusieurs algorithmes

```python
from envs.lineworld import LineWorld
from algos.q_learning import QLearningAgent
from algos.sarsa import SARSAAgent
from algos.monte_carlo import OnPolicyMonteCarlo

env = LineWorld(length=10)

algorithms = {
    'Q-Learning': QLearningAgent(env, alpha=0.1, gamma=0.99, epsilon=0.1),
    'SARSA': SARSAAgent(env, alpha=0.1, gamma=0.99, epsilon=0.1),
    'Monte Carlo': OnPolicyMonteCarlo(env, gamma=0.99, epsilon=0.1)
}

results = {}
for name, agent in algorithms.items():
    print(f"\nTraining {name}...")
    agent.train(num_episodes=1000, verbose=False)
    eval_results = agent.evaluate(num_episodes=100)
    results[name] = eval_results['mean_reward']
    print(f"{name}: {eval_results['mean_reward']:.2f}")

# Afficher le meilleur
best = max(results, key=results.get)
print(f"\nMeilleur algorithme: {best} ({results[best]:.2f})")
```

### √âtude d'hyperparam√®tres

```python
from envs.gridworld import GridWorld
from algos.q_learning import QLearningAgent
import numpy as np

env = GridWorld(width=5, height=5)

# Tester diff√©rentes valeurs d'alpha
alphas = [0.01, 0.05, 0.1, 0.2, 0.5]
results = {}

for alpha in alphas:
    agent = QLearningAgent(env, alpha=alpha, gamma=0.99, epsilon=0.1)
    agent.train(num_episodes=1000, verbose=False)
    eval_results = agent.evaluate(num_episodes=100)
    results[alpha] = eval_results['mean_reward']
    print(f"Alpha {alpha}: {eval_results['mean_reward']:.2f}")

# Trouver le meilleur alpha
best_alpha = max(results, key=results.get)
print(f"\nMeilleur alpha: {best_alpha} ({results[best_alpha]:.2f})")
```

### Visualiser l'apprentissage

```python
import matplotlib.pyplot as plt
from envs.lineworld import LineWorld
from algos.q_learning import QLearningAgent

env = LineWorld(length=10)
agent = QLearningAgent(env, alpha=0.1, gamma=0.99, epsilon=0.1)

# Entra√Æner
agent.train(num_episodes=1000, verbose=False)

# Visualiser les rewards par √©pisode
plt.figure(figsize=(10, 6))
plt.plot(agent.episode_rewards)
plt.xlabel('Episode')
plt.ylabel('Reward')
plt.title('Q-Learning on Line World - Training Progress')
plt.grid(True)
plt.show()

# Moyenne mobile sur 100 √©pisodes
window = 100
moving_avg = np.convolve(agent.episode_rewards, np.ones(window)/window, mode='valid')
plt.figure(figsize=(10, 6))
plt.plot(moving_avg)
plt.xlabel('Episode')
plt.ylabel('Average Reward (100 episodes)')
plt.title('Q-Learning - Moving Average')
plt.grid(True)
plt.show()
```

## üìà Interpr√©tation des r√©sultats

### M√©triques importantes

1. **Mean Reward**: R√©compense moyenne par √©pisode
   - Plus √©lev√© = meilleur
   - Comparez entre algorithmes

2. **Success Rate**: Taux de succ√®s (√©pisodes avec reward > 0)
   - Plus √©lev√© = meilleur
   - Indique la fiabilit√© de l'algorithme

3. **Convergence Episode**: √âpisode o√π l'algorithme a converg√©
   - Plus t√¥t = meilleur
   - Indique la vitesse d'apprentissage

4. **Training Time**: Temps d'entra√Ænement
   - Comparez l'efficacit√© computationnelle

### Quand utiliser quel algorithme?

#### Dynamic Programming (Policy/Value Iteration)
- ‚úÖ **Quand**: Mod√®le de l'environnement disponible, √©tats discrets
- ‚ùå **Quand**: Grands espaces d'√©tats, pas de mod√®le
- **Environnements**: Line World, Grid World (petits)

#### Monte Carlo
- ‚úÖ **Quand**: √âpisodes complets disponibles, pas besoin de mod√®le
- ‚ùå **Quand**: √âpisodes tr√®s longs, besoin d'apprentissage rapide
- **Environnements**: Tous (surtout RPS, Monty Hall)

#### TD Learning (SARSA, Q-Learning, Expected SARSA)
- ‚úÖ **Quand**: Apprentissage en ligne, pas de mod√®le n√©cessaire
- ‚ùå **Quand**: Besoin de convergence tr√®s rapide
- **Environnements**: Tous (surtout Line World, Grid World)

#### Planning (Dyna-Q, Dyna-Q+)
- ‚úÖ **Quand**: Mod√®le peut √™tre appris, besoin d'efficacit√©
- ‚ùå **Quand**: Environnement non-stationnaire (sauf Dyna-Q+)
- **Environnements**: Line World, Grid World

### Conseils pour le choix d'hyperparam√®tres

1. **Gamma (facteur d'actualisation)**
   - Proche de 1 (0.99): Privil√©gie les r√©compenses futures
   - Proche de 0 (0.5): Privil√©gie les r√©compenses imm√©diates
   - **Recommandation**: 0.9-0.99 pour la plupart des cas

2. **Alpha (taux d'apprentissage)**
   - Trop √©lev√© (>0.5): Instabilit√©
   - Trop faible (<0.01): Apprentissage lent
   - **Recommandation**: 0.1 pour commencer, ajuster selon les r√©sultats

3. **Epsilon (exploration)**
   - Trop √©lev√© (>0.3): Trop d'exploration, peu d'exploitation
   - Trop faible (<0.01): Peu d'exploration
   - **Recommandation**: 0.1, peut √™tre r√©duit progressivement

4. **N Planning Steps (Dyna-Q)**
   - Plus √©lev√©: Plus de planning, mais plus co√ªteux
   - **Recommandation**: 5-10 pour commencer

## üîß D√©pannage

### Erreurs communes

1. **"action_space not found"**
   - Solution: V√©rifiez que l'environnement a `n_actions()` ou `action_space`

2. **"State key error"**
   - Solution: Les √©tats doivent √™tre hashables (tuples, pas listes)

3. **"Convergence lente"**
   - Solution: Ajustez les hyperparam√®tres (alpha, epsilon, gamma)

4. **"M√©moire insuffisante"**
   - Solution: R√©duisez la taille de l'environnement ou le nombre d'√©pisodes

## üìù Notes importantes

1. **Sauvegarde**: Utilisez `agent.save(path)` pour sauvegarder les agents entra√Æn√©s
2. **Chargement**: Utilisez `agent.load(path)` pour charger un agent sauvegard√©
3. **√âvaluation**: Toujours √©valuer en mode `training=False` pour des r√©sultats fiables
4. **Reproductibilit√©**: Utilisez `random.seed()` pour des r√©sultats reproductibles

## üéØ Prochaines √©tapes

1. Tester tous les algorithmes sur tous les environnements
2. Comparer les performances
3. √âtudier l'impact des hyperparam√®tres
4. Analyser les politiques apprises
5. Pr√©parer le rapport et la soutenance

Bon apprentissage ! üöÄ



