# ğŸ† Meilleur Algorithme par Environnement

## ğŸ“Š RÃ©sultats des Tests

### 1. LineWorldSimple

**ğŸ† Meilleur : SARSA** (100% success, -4.90 reward)

| Algorithme | Success Rate | Mean Reward | Rang |
|-----------|--------------|-------------|------|
| **SARSA** | **100%** | **-4.90** | ğŸ¥‡ |
| Expected SARSA | 100% | -4.90 | ğŸ¥ˆ |
| Dyna-Q | 100% | -4.90 | ğŸ¥‰ |
| Policy Iteration | 100% | -4.90 | 4 |
| Value Iteration | 100% | -4.90 | 5 |
| Q-Learning | 100% | -5.40 | 6 |

**Analyse** : Tous les algorithmes atteignent 100% success. SARSA, Expected SARSA et Dyna-Q ont le meilleur reward (-4.90), ce qui signifie qu'ils trouvent un chemin plus court vers le goal.

---

### 2. GridWorldSimple

**ğŸ† Meilleur : Q-Learning** (100% success, -3.00 reward)

| Algorithme | Success Rate | Mean Reward | Rang |
|-----------|--------------|-------------|------|
| **Q-Learning** | **100%** | **-3.00** | ğŸ¥‡ |
| SARSA | 100% | -3.00 | ğŸ¥ˆ |
| Expected SARSA | 100% | -3.00 | ğŸ¥‰ |
| Dyna-Q | 100% | -3.00 | 4 |
| Policy Iteration | 100% | -3.00 | 5 |
| Value Iteration | 100% | -3.00 | 6 |

**Analyse** : Tous les algorithmes atteignent 100% success avec le mÃªme reward. Q-Learning est choisi comme meilleur car c'est l'algorithme off-policy le plus standard et le plus rapide Ã  converger.

---

### 3. TwoRoundRPS

**ğŸ† Meilleur : Expected SARSA** (68% success, +1.06 reward)

| Algorithme | Success Rate | Mean Reward | Rang |
|-----------|--------------|-------------|------|
| **Expected SARSA** | **68%** | **+1.06** | ğŸ¥‡ |
| Q-Learning | 66% | +1.04 | ğŸ¥ˆ |
| SARSA | 66% | +0.92 | ğŸ¥‰ |
| Dyna-Q | 54% | +0.84 | 4 |

**Analyse** : Expected SARSA performe le mieux sur ce jeu compÃ©titif. C'est normal car Expected SARSA utilise la valeur attendue de Q pour le prochain Ã©tat, ce qui est plus stable pour les environnements stochastiques comme RPS.

**Note** : 68% est un excellent rÃ©sultat pour un jeu compÃ©titif oÃ¹ l'adversaire s'adapte.

---

### 4. MontyHallLevel1

**ğŸ† Meilleur : SARSA** (64% success, +0.64 reward)

| Algorithme | Success Rate | Mean Reward | Rang |
|-----------|--------------|-------------|------|
| **SARSA** | **64%** | **+0.64** | ğŸ¥‡ |
| Q-Learning | 52% | +0.52 | ğŸ¥ˆ |
| Dyna-Q | 50% | +0.50 | ğŸ¥‰ |
| Expected SARSA | 46% | +0.46 | 4 |

**Analyse** : SARSA (on-policy) performe mieux que Q-Learning (off-policy) sur MontyHall. Cela s'explique car SARSA apprend la politique qu'il suit, ce qui est plus adaptÃ© pour ce problÃ¨me probabiliste sÃ©quentiel.

**Note** : La stratÃ©gie optimale thÃ©orique est de toujours changer (66% win rate). SARSA avec 64% s'en approche bien.

---

## ğŸ“‹ RÃ©sumÃ© Global

| Environnement | Meilleur Algorithme | Success Rate | Pourquoi |
|--------------|---------------------|--------------|----------|
| **LineWorldSimple** | **SARSA** | 100% | Meilleur reward (-4.90) |
| **GridWorldSimple** | **Q-Learning** | 100% | Standard, rapide, tous Ã©gaux |
| **TwoRoundRPS** | **Expected SARSA** | 68% | Plus stable pour stochastique |
| **MontyHallLevel1** | **SARSA** | 64% | On-policy mieux pour sÃ©quentiel |

---

## ğŸ’¡ InterprÃ©tation

### Pourquoi ces algorithmes sont meilleurs ?

1. **LineWorldSimple & GridWorldSimple** :
   - Tous les algorithmes atteignent 100% success
   - Les diffÃ©rences sont minimes (rÃ©compenses lÃ©gÃ¨rement diffÃ©rentes)
   - **SARSA/Q-Learning** sont les plus standards et rapides

2. **TwoRoundRPS** :
   - **Expected SARSA** : Utilise la valeur attendue, plus stable pour les environnements stochastiques
   - Meilleure gestion de l'incertitude dans les actions de l'adversaire

3. **MontyHallLevel1** :
   - **SARSA** : On-policy, apprend la politique qu'il suit
   - Plus adaptÃ© pour les problÃ¨mes sÃ©quentiels avec dÃ©cisions dÃ©pendantes

---

## ğŸ¯ Recommandations pour la Soutenance

### DÃ©monstrations Principales

1. **LineWorldSimple avec SARSA** : 100% success, meilleur reward
2. **GridWorldSimple avec Q-Learning** : 100% success, algorithme standard
3. **TwoRoundRPS avec Expected SARSA** : 68% success (excellent pour compÃ©titif)
4. **MontyHall avec SARSA** : 64% success (proche de l'optimal 66%)

### Points Ã  Mentionner

- **LineWorldSimple & GridWorldSimple** : Tous les algorithmes fonctionnent (100% success)
- **TwoRoundRPS** : Expected SARSA meilleur grÃ¢ce Ã  sa stabilitÃ© pour les environnements stochastiques
- **MontyHall** : SARSA (on-policy) mieux adaptÃ© pour les dÃ©cisions sÃ©quentielles

---

**Date** : 2025-01-01
**Status** : âœ… Tests complets effectuÃ©s

