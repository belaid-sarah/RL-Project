# Test des Algorithmes sur les Environnements Secrets

Ce document explique comment tester tous les algorithmes de RL sur les environnements secrets (SecretEnv0, SecretEnv1, SecretEnv2, SecretEnv3).

## Pr√©requis

### 1. Biblioth√®ques DLL/SO

Les environnements secrets n√©cessitent des biblioth√®ques natives qui doivent √™tre plac√©es dans le dossier `libs/` :

- **Windows**: `libs/secret_envs.dll`
- **Linux**: `libs/libsecret_envs.so`
- **macOS Intel**: `libs/libsecret_envs_intel_macos.dylib`
- **macOS Apple Silicon**: `libs/libsecret_envs.dylib`

Ces fichiers sont normalement fournis par l'enseignant. Si vous ne les avez pas, cr√©ez le dossier `libs/` et placez-y les fichiers appropri√©s.

### 2. Structure du projet

```
RL-PROJECT/
‚îú‚îÄ‚îÄ libs/                    # Dossier pour les DLL/SO (√† cr√©er si n√©cessaire)
‚îÇ   ‚îú‚îÄ‚îÄ secret_envs.dll      # Windows
‚îÇ   ‚îú‚îÄ‚îÄ libsecret_envs.so    # Linux
‚îÇ   ‚îî‚îÄ‚îÄ libsecret_envs.dylib # macOS
‚îú‚îÄ‚îÄ envs/
‚îÇ   ‚îú‚îÄ‚îÄ secret_envs_wrapper.py  # Wrapper fourni par l'enseignant
‚îÇ   ‚îî‚îÄ‚îÄ secret_env_adapter.py   # Adaptateur cr√©√© pour notre interface
‚îú‚îÄ‚îÄ algos/                   # Tous nos algorithmes
‚îî‚îÄ‚îÄ test_secret_envs.py     # Script de test principal
```

## Utilisation

### Tester tous les algorithmes sur tous les environnements secrets

```bash
python test_secret_envs.py --all
```

Cette commande va :
1. Tester chaque algorithme sur chaque environnement secret
2. Sauvegarder les r√©sultats dans `results/`
3. Afficher un r√©sum√© avec le meilleur algorithme pour chaque environnement

### Tester un algorithme sp√©cifique sur un environnement sp√©cifique

```bash
python test_secret_envs.py --algo Q-Learning --env SecretEnv0 --episodes 2000
```

### Options disponibles

- `--all`: Ex√©cute tous les tests
- `--algo <nom>`: Nom de l'algorithme √† tester
- `--env <nom>`: Nom de l'environnement √† tester
- `--episodes <n>`: Nombre d'√©pisodes d'entra√Ænement (d√©faut: 2000)
- `--verbose`: Affiche les d√©tails pendant l'entra√Ænement

### Algorithmes disponibles

- `PolicyIteration`
- `ValueIteration`
- `MonteCarloES`
- `OnPolicyMonteCarlo`
- `OffPolicyMonteCarlo`
- `SARSA`
- `Q-Learning`
- `ExpectedSARSA`
- `Dyna-Q`
- `Dyna-Q+`

### Environnements disponibles

- `SecretEnv0`
- `SecretEnv1`
- `SecretEnv2`
- `SecretEnv3`

## R√©sultats

Les r√©sultats sont sauvegard√©s dans le dossier `results/` :

1. **Fichiers individuels**: Un fichier JSON par combinaison algorithme/environnement
   - Format: `{Algorithme}_{Environnement}_{Timestamp}.json`

2. **Rapport complet**: Un fichier JSON avec tous les r√©sultats
   - Format: `secret_envs_complete_report_{Timestamp}.json`

### Structure des r√©sultats

Chaque fichier JSON contient :
- `algorithm`: Nom de l'algorithme
- `environment`: Nom de l'environnement
- `hyperparameters`: Hyperparam√®tres utilis√©s
- `training`: Statistiques d'entra√Ænement (temps, √©pisodes, convergence)
- `evaluation`: Statistiques d'√©valuation (mean reward, success rate, steps)
- `success`: Si le test a r√©ussi
- `error`: Message d'erreur si √©chec

## Identification du meilleur algorithme

Le script affiche automatiquement :
- Un r√©sum√© par environnement avec les algorithmes tri√©s par performance
- Le meilleur algorithme pour chaque environnement (üèÜ)
- Les m√©triques cl√©s : mean reward, success rate, mean steps, training time

## Exemple de sortie

```
================================================================
SUMMARY - SECRET ENVIRONMENTS
================================================================

SecretEnv0:
------------------------------------------------------------
üèÜ  1. Q-Learning          | Reward:  45.23 ¬±  2.15 | Success:  95.0% | Steps:   12.3 | Time:  15.23s
    2. SARSA              | Reward:  43.12 ¬±  2.45 | Success:  92.0% | Steps:   13.1 | Time:  14.87s
    3. Dyna-Q             | Reward:  42.89 ¬±  2.67 | Success:  91.0% | Steps:   13.5 | Time:  18.45s
...

================================================================
üèÜ MEILLEURS ALGORITHMES PAR ENVIRONNEMENT
================================================================

SecretEnv0:
  ü•á Meilleur algorithme: Q-Learning
     - Mean Reward: 45.23 ¬± 2.15
     - Success Rate: 95.0%
     - Mean Steps: 12.3
     - Training Time: 15.23s
```

## D√©pannage

### Erreur: "Could not find module 'libs/secret_envs.dll'"

**Solution**: V√©rifiez que :
1. Le dossier `libs/` existe
2. Le fichier DLL/SO appropri√© est pr√©sent dans `libs/`
3. Le nom du fichier correspond √† votre syst√®me d'exploitation

### Erreur: "Environnement non supporte pour Policy Iteration"

**Solution**: Les environnements secrets sont maintenant support√©s gr√¢ce √† l'adaptateur. Si cette erreur persiste, v√©rifiez que `secret_env_adapter.py` est bien pr√©sent dans `envs/`.

### Les tests sont tr√®s longs

**Solution**: R√©duisez le nombre d'√©pisodes dans `CONFIG['num_episodes']` dans `test_secret_envs.py`.

## Notes techniques

### Adaptateur SecretEnvAdapter

L'adaptateur `SecretEnvAdapter` convertit l'interface des environnements secrets vers notre interface `BaseEnv` standard :

- `reset()` ‚Üí retourne l'√©tat initial
- `step(action)` ‚Üí retourne `(next_state, reward, done, info)`
- `sample_action()` ‚Üí retourne une action al√©atoire
- `action_space` ‚Üí liste des actions possibles

### Support des algorithmes bas√©s sur mod√®le

Policy Iteration et Value Iteration utilisent les m√©thodes MDP des environnements secrets :
- `num_states()`: nombre d'√©tats
- `num_actions()`: nombre d'actions
- `p(s, a, s_p, r_index)`: probabilit√© de transition
- `reward(r_index)`: valeur du reward

Ces m√©thodes permettent de construire le mod√®le MDP sans exploration.

