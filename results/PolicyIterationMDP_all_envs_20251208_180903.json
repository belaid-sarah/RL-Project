[
  {
    "algorithm": "PolicyIterationMDP",
    "environment": "LineWorldSimple",
    "hyperparameters": {
      "gamma": 0.99,
      "theta": 1e-06
    },
    "num_episodes": 20,
    "training_time": 0.03693103790283203,
    "evaluation": {
      "mean_reward": -18.646396898727026,
      "std_reward": 12.282256972425591,
      "min_reward": -52.1,
      "max_reward": -5.5838609607069865,
      "mean_steps": 20.0,
      "std_steps": 0.0,
      "success_rate": 0.0,
      "total_episodes": 100
    }
  },
  {
    "algorithm": "PolicyIterationMDP",
    "environment": "GridWorld",
    "hyperparameters": {
      "gamma": 0.99,
      "theta": 1e-06
    },
    "num_episodes": 30,
    "training_time": 0.4183173179626465,
    "evaluation": {
      "mean_reward": -65.0649678102905,
      "std_reward": 47.24138934913907,
      "min_reward": -190.495246448841,
      "max_reward": 23.916323114935192,
      "mean_steps": 47.06,
      "std_steps": 10.71617469062538,
      "success_rate": 0.07,
      "total_episodes": 100
    }
  },
  {
    "algorithm": "PolicyIterationMDP",
    "environment": "TwoRoundRPS",
    "hyperparameters": {
      "gamma": 0.99,
      "theta": 1e-06
    },
    "num_episodes": 20,
    "training_time": 0.007425069808959961,
    "evaluation": {
      "mean_reward": 0.75,
      "std_reward": 1.4097872179871684,
      "min_reward": "-2",
      "max_reward": "2",
      "mean_steps": 2.0,
      "std_steps": 0.0,
      "success_rate": 0.71,
      "total_episodes": 100
    }
  }
]